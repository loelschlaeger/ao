---
title: "ao"
author: "Lennart OelschlÃ¤ger"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{ao}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
bibliography: ref.bib
link-citations: true
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(ao)
```

## Theory

### What is alternating optimization?

Alternating optimization is an iterative procedure for optimizing some function jointly over all variables by alternating restricted optimization over individual variable subsets.

More precisely, consider minimizing or maximizing $f:\mathbb{R}^n \to \mathbb{R}$ over the set of feasible points $x \in X \subseteq \mathbb{R}^n$. The underlying algorithm of alternating optimization is as follows:

1. Assign an initial value for $x$.

2. Optimize $f$ with respect to a subset of parameters $\tilde{x}$ while holding the other parameters constant. This can be done explicitly or implicitly (by numerical optimization, which is the implemented method in this package).

3. Replace the values in $x$ by the optimal values for $\tilde{x}$ found in step 2.

4. Repeat from step 2 with another parameter subset.

5. Stop when the process has converged or reached an iteration limit.

Note that alternating optimization is a generalization of joint optimization, where the only parameter subset would be the whole set of parameters.

### When is alternating optimization a good idea?

- When the joint optimization is (numerically) difficult (or not feasible).

- When there is a natural division of parameters in subsets. That is the case e.g. for likelihood functions, where the parameter space naturally divides into parameter subsets corresponding to linear effects, variances and covariances with different influence on the likelihood value.

- To improve optimization time in some cases, see [@hu2002] for an example.

- Compared to joint optimization, alternating optimization may be better in bypassing local optima, see [@bezdek2002].

### What are the properties of alternating optimization?

Alternating optimization, under certain conditions on $f$, can convergence to the global optimum. However, the set of possible solutions also contains saddle points of $f$, see for example [@bezdek1987].

[@bezdek2003] shows, that alternating optimization under reasonable assumptions is locally q-linearly convergent.

## Application

How are the functions and arguments defined?

Example 1

```{r example 1}
f <- set_f(f = function(x) x[1]^2 - 3*x[1]*x[2] + x[2]^2, npar = 2)
ao(f = f, initial = -1, partition = list(1,2))
ao(f = f, initial = 1, partition = list(c(1,2)))
```

Example 2

```{r example 2}
A <- matrix(c(100, 80, 5, 1, 80, 90, 2, 1, 5, 2, 70, 40, 1, 1, 40, 80), 4, 4)
q <- function(x) as.numeric(t(x) %*% A %*% x)
f <- set_f(f = q, npar = 4)
ao(f = f, initial = 1, partition = list(1:2,3:4), iterations = 100)
ao(f = f, initial = 1, partition = list(1:4), iterations = 100)
```

Example from Mail

```{r example 3}
valley <- function(x) {
  cons <- c(1.003344481605351, -3.344481605351171e-03)
  n <- length(x)
  f <- rep(0, n)
  j <- 3 * (1:(n/3))
  jm2 <- j - 2
  jm1 <- j - 1
  f[jm2] <- (cons[2]*x[jm2]^3 + cons[1]*x[jm2]) * exp(-(x[jm2]^2)/100) - 1
  f[jm1] <- 10 * (sin(x[jm2]) - x[jm1])
  f[j] <- 10 * (cos(x[jm2]) - x[j])
  sum(f*f)
}
f <- set_f(f = valley, npar = 9, lower = 0, upper = 10, check = FALSE)
ao(f = f, partition = list(1, 2, 3, 4, 5, 6, 7, 8, 9), initial = 0, iterations = 3)
```

## References
